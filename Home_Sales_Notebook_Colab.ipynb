# Install Java and Spark
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -qO - "https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz" | tar xzf -

# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.3-bin-hadoop3"

# Install findspark
!pip install -q findspark

# Initialize Spark
import findspark
findspark.init()
from pyspark.sql import SparkSession
import time

# Start Spark Session
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()

# Load dataset from AWS S3
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"
spark.sparkContext.addFile(url)

# Read CSV into DataFrame
df = spark.read.csv(SparkFiles.get("home_sales_revised.csv"), header=True, inferSchema=True)

# Create a temporary view
df.createOrReplaceTempView("home_sales")

# Average price for four-bedroom homes sold per year
spark.sql("""
SELECT YEAR(date_sold) AS year, 
       ROUND(AVG(price), 2) AS average_price
FROM home_sales
WHERE bedrooms = 4
GROUP BY year
ORDER BY year
""").show()

#  Average price of homes with 3 bedrooms & 3 bathrooms by year built
spark.sql("""
SELECT date_built, 
       ROUND(AVG(price), 2) AS average_price
FROM home_sales
WHERE bedrooms = 3 AND bathrooms = 3
GROUP BY date_built
ORDER BY date_built
""").show()

# Average price of homes with specific conditions
spark.sql("""
SELECT date_built, 
       ROUND(AVG(price), 2) AS average_price
FROM home_sales
WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000
GROUP BY date_built
ORDER BY date_built
""").show()

# Average price of homes per "view" rating where avg price â‰¥ $350,000
start_time = time.time()

spark.sql("""
SELECT view, 
       ROUND(AVG(price), 2) AS average_price
FROM home_sales
GROUP BY view
HAVING average_price >= 350000
ORDER BY view DESC
""").show()

print(f"Execution Time (Uncached): {time.time() - start_time:.5f} seconds")

# Cache the temporary table
spark.sql("CACHE TABLE home_sales")

# Verify if caching was successful
print("Is 'home_sales' cached? ", spark.catalog.isCached("home_sales"))

# Run cached version of previous query
start_time = time.time()

spark.sql("""
SELECT view, 
       ROUND(AVG(price), 2) AS average_price
FROM home_sales
GROUP BY view
HAVING average_price >= 350000
ORDER BY view DESC
""").show()

print(f"Execution Time (Cached): {time.time() - start_time:.5f} seconds")

# Partition data by "date_built" and save as Parquet
df.write.partitionBy("date_built").mode("overwrite").parquet("home_sales_partitioned.parquet")

# Read the Parquet formatted data
parquet_df = spark.read.parquet("home_sales_partitioned.parquet")

#  Create a temporary view for the Parquet data
parquet_df.createOrReplaceTempView("home_sales_parquet")

# Run query again using Parquet table
start_time = time.time()

spark.sql("""
SELECT view, 
       ROUND(AVG(price), 2) AS average_price
FROM home_sales_parquet
GROUP BY view
HAVING average_price >= 350000
ORDER BY view DESC
""").show()

print(f"Execution Time (Parquet): {time.time() - start_time:.5f} seconds")

#  Uncache the temporary table
spark.sql("UNCACHE TABLE home_sales")

# Verify if the table was uncached
print("Is 'home_sales' cached after uncaching? ", spark.catalog.isCached("home_sales"))
